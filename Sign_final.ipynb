{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69957cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Constants\n",
    "IMAGE_SIZE = (300, 300)\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 20\n",
    "NUM_CLASSES = 26  # 26 classes\n",
    "DATASET_PATH = \"/suryas/gb_S/final_project/\"\n",
    "\n",
    "# Load dataset\n",
    "def load_dataset(dataset_path):\n",
    "    images = []\n",
    "    labels = []\n",
    "    label_mapping = {}\n",
    "    \n",
    "    class_folders = sorted(os.listdir(dataset_path))\n",
    "    for label, category in enumerate(class_folders):\n",
    "        label_mapping[label] = category\n",
    "        category_path = os.path.join(dataset_path, category)\n",
    "        \n",
    "        for image_name in os.listdir(category_path):\n",
    "            image_path = os.path.join(category_path, image_name)\n",
    "            image = tf.keras.preprocessing.image.load_img(image_path, target_size=IMAGE_SIZE)\n",
    "            image = tf.keras.preprocessing.image.img_to_array(image)\n",
    "            images.append(image)\n",
    "            labels.append(label)\n",
    "    \n",
    "    return np.array(images), np.array(labels), label_mapping\n",
    "\n",
    "# Split dataset into training and testing sets\n",
    "def split_dataset(images, labels):\n",
    "    x_train, x_test, y_train, y_test = train_test_split(images, labels, test_size=0.2, random_state=42)\n",
    "    return x_train, x_test, y_train, y_test\n",
    "\n",
    "# Data augmentation and normalization\n",
    "def create_train_generator(x_train):\n",
    "    train_datagen = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        rotation_range=20,\n",
    "        width_shift_range=0.2,\n",
    "        height_shift_range=0.2,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True,\n",
    "        fill_mode='nearest'\n",
    "    )\n",
    "    train_datagen.fit(x_train)\n",
    "    return train_datagen\n",
    "\n",
    "# Define model architecture with dropout\n",
    "\n",
    "def create_model(input_shape, num_classes):\n",
    "    model = models.Sequential()\n",
    "      # Block 1\n",
    "    model.add(Conv2D(64, (3, 3), activation='relu', padding='same', input_shape=input_shape))\n",
    "    model.add(Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
    "    model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "\n",
    "    # Block 2\n",
    "    model.add(Conv2D(128, (3, 3), activation='relu', padding='same'))\n",
    "    model.add(Conv2D(128, (3, 3), activation='relu', padding='same'))\n",
    "    model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "\n",
    "    # Block 3\n",
    "    model.add(Conv2D(256, (3, 3), activation='relu', padding='same'))\n",
    "    model.add(Conv2D(256, (3, 3), activation='relu', padding='same'))\n",
    "    model.add(Conv2D(256, (3, 3), activation='relu', padding='same'))\n",
    "    model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "\n",
    "    # Block 4\n",
    "    model.add(Conv2D(512, (3, 3), activation='relu', padding='same'))\n",
    "    model.add(Conv2D(512, (3, 3), activation='relu', padding='same'))\n",
    "    model.add(Conv2D(512, (3, 3), activation='relu', padding='same'))\n",
    "    model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "\n",
    "    # Block 5\n",
    "    model.add(Conv2D(512, (3, 3), activation='relu', padding='same'))\n",
    "    model.add(Conv2D(512, (3, 3), activation='relu', padding='same'))\n",
    "    model.add(Conv2D(512, (3, 3), activation='relu', padding='same'))\n",
    "    model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "\n",
    "    # Flatten\n",
    "    model.add(Flatten())\n",
    "\n",
    "    # Fully connected layers\n",
    "    model.add(Dense(4096, activation='relu'))\n",
    "    model.add(Dense(4096, activation='relu'))\n",
    "    model.add(Dense(num_classes, activation='softmax')) \n",
    "    return model\n",
    "\n",
    "# training process\n",
    "def train_model(model, train_generator, x_train, y_train, x_test, y_test, batch_size, epochs):\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "    # learning rate is changed\n",
    "    model.compile(optimizer = optimizer,\n",
    "                  loss='sparse_categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    history = model.fit(train_generator.flow(x_train, y_train, batch_size=batch_size),\n",
    "                        steps_per_epoch=len(x_train) // batch_size,\n",
    "                        epochs=epochs,\n",
    "                        validation_data=(x_test, y_test))\n",
    "    \n",
    "    return model, history\n",
    "\n",
    "# Save label mapping to text file\n",
    "def save_label_mapping(label_mapping, file_path):\n",
    "    with open(file_path, \"w\") as file:\n",
    "        for label, category in label_mapping.items():\n",
    "            file.write(f\"{label}: {category}\\n\")\n",
    "\n",
    "            \n",
    "# Load dataset\n",
    "images, labels, label_mapping = load_dataset(DATASET_PATH)\n",
    "\n",
    "# Split dataset into training and testing sets\n",
    "x_train, x_test, y_train, y_test = split_dataset(images, labels)\n",
    "\n",
    "# Create data generator for training\n",
    "train_datagen = create_train_generator(x_train)\n",
    "\n",
    "# Define model\n",
    "model = create_model(input_shape=(*IMAGE_SIZE, 3), num_classes=NUM_CLASSES)\n",
    "\n",
    "# Train model\n",
    "trained_model, history = train_model(model, train_datagen, x_train, y_train, x_test, y_test, BATCH_SIZE, EPOCHS)\n",
    "\n",
    "# Save trained model\n",
    "trained_model.save(\"trained_model.h5\")\n",
    "\n",
    "# Save label mapping to text file\n",
    "save_label_mapping(label_mapping, \"label_mapping.txt\")\n",
    "\n",
    "# test accuracy\n",
    "test_loss, test_accuracy = trained_model.evaluate(x_test, y_test)\n",
    "print(f\"Test Accuracy: {test_accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da7b1491",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import cv2\n",
    "import numpy as np\n",
    "import math\n",
    "import time\n",
    "import os\n",
    "\n",
    "# Import HandTrackingModule from cvzone\n",
    "from cvzone.HandTrackingModule import HandDetector\n",
    "\n",
    "# Initialize camera and hand detector\n",
    "cap = cv2.VideoCapture(0)\n",
    "detector = HandDetector(maxHands=1)\n",
    "\n",
    "# Constants\n",
    "offset = 20  # Offset to expand hand bounding box\n",
    "imgSize = 300  # Size of the square image to be saved\n",
    "counter = 0  # Counter to keep track of saved images\n",
    "num_images = 300  # Total number of images to capture\n",
    "folder = \"Signs/a2\"  # Folder to save images\n",
    "\n",
    "# Create folder if it doesn't exist\n",
    "if not os.path.exists(folder):\n",
    "    os.makedirs(folder)\n",
    "\n",
    "# Main loop to capture and process images\n",
    "while True:\n",
    "    # Capture frame from camera\n",
    "    success, img = cap.read()\n",
    "    \n",
    "    # Detect hands in the frame\n",
    "    hands, img = detector.findHands(img, draw=True)\n",
    "    \n",
    "    # Process if hands are detected\n",
    "    if hands:\n",
    "        hand = hands[0]\n",
    "        \n",
    "        # Get bounding box coordinates of the hand\n",
    "        x, y, w, h = hand['bbox']\n",
    "        \n",
    "        # Check if the hand is within the image boundaries\n",
    "        if x - offset >= 0 and y - offset >= 0 and x + w + offset < img.shape[1] and y + h + offset < img.shape[0]:\n",
    "            # Create a canvas to place the processed hand image\n",
    "            canvas = np.ones((imgSize, imgSize, 3), np.uint8) * 255\n",
    "            \n",
    "            # Crop and resize the hand image to fit the canvas\n",
    "            imgCrop = img[y - offset:y + h + offset, x - offset:x + w + offset]\n",
    "            aspectRatio = h / w\n",
    "            \n",
    "            # Resize and center the hand image within canvas\n",
    "            if aspectRatio > 1:\n",
    "                k = imgSize / h\n",
    "                wCal = math.ceil(k * w)\n",
    "                imgResize = cv2.resize(imgCrop, (wCal, imgSize))\n",
    "                wGap = math.ceil((imgSize - wCal) / 2)\n",
    "                canvas[:, wGap:wCal + wGap] = imgResize\n",
    "            else:\n",
    "                k = imgSize / w\n",
    "                hCal = math.ceil(k * h)\n",
    "                imgResize = cv2.resize(imgCrop, (imgSize, hCal))\n",
    "                hGap = math.ceil((imgSize - hCal) / 2)\n",
    "                canvas[hGap:hCal + hGap, :] = imgResize\n",
    "            \n",
    "            # Display cropped hand and processed canvas\n",
    "            cv2.imshow(\"ImageCrop\", imgCrop)\n",
    "            cv2.imshow(\"Canvas\", canvas)\n",
    "    \n",
    "    # Display original frame\n",
    "    cv2.imshow(\"Image\", img)\n",
    "    \n",
    "    # Wait for key press\n",
    "    key = cv2.waitKey(1)\n",
    "    \n",
    "    # Save the processed image if 'S' key (key 32) is pressed\n",
    "    if key == 32:  # 'S' key\n",
    "        if counter < num_images:\n",
    "            counter += 1\n",
    "            # Save image with timestamp as filename\n",
    "            cv2.imwrite(f'{folder}/Image_{time.time()}.jpg', canvas)\n",
    "            print(counter)\n",
    "        else:\n",
    "            print(f\"Count {num_images} reached. Change to next character.\")\n",
    "            break\n",
    "    \n",
    "    # Stop the process if 'q' key is pressed\n",
    "    if key == ord(\"q\"):\n",
    "        break\n",
    "\n",
    "# Release camera and close all OpenCV windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b60635ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "1/1 [==============================] - 1s 723ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n"
     ]
    }
   ],
   "source": [
    "import tkinter as tk\n",
    "import cv2\n",
    "from cvzone.HandTrackingModule import HandDetector\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import math\n",
    "from PIL import Image, ImageTk\n",
    "\n",
    "# Initialize camera and hand detector\n",
    "cap = cv2.VideoCapture(0)\n",
    "detector = HandDetector(maxHands=1)\n",
    "\n",
    "# Constants\n",
    "offset = 30  # Offset to expand hand bounding box\n",
    "imgSize = 224  # Size of the square image for gesture prediction\n",
    "IMAGE_SIZE = (224, 224)  # Image size expected by the model\n",
    "FPS = 10  # Frames per second for video stream\n",
    "\n",
    "# Load the trained model for gesture prediction\n",
    "model = tf.keras.models.load_model('Trained_models/latest_models/keras_model_S.h5')\n",
    "\n",
    "# Read the class labels from the file\n",
    "with open('Trained_models/latest_models/labels_W.txt', 'r') as file:\n",
    "    labels_list = [line.strip() for line in file.readlines()]\n",
    "\n",
    "def preprocess_image(img):\n",
    "    # Convert the image from BGR to RGB\n",
    "    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    # Normalize pixel values to range [0, 1]\n",
    "    img_normalized = img_rgb / 255.0\n",
    "    # Resize the image to match the model's input size\n",
    "    resized_img = cv2.resize(img_normalized, IMAGE_SIZE)\n",
    "    return resized_img\n",
    "\n",
    "def predict_gesture(hand_img):\n",
    "    preprocessed_img = preprocess_image(hand_img)\n",
    "    # Make a prediction using the loaded model\n",
    "    predictions = model.predict(np.array([preprocessed_img]))\n",
    "    # Get the index of the predicted class\n",
    "    predicted_class_index = np.argmax(predictions)\n",
    "    return predicted_class_index\n",
    "\n",
    "def show_frames():\n",
    "    # Read frame from camera\n",
    "    success, img = cap.read()\n",
    "    frame = cv2.flip(img, 1)  # Flip frame horizontally for mirror view\n",
    "    \n",
    "    # Detect hands in the frame\n",
    "    hands, img = detector.findHands(img)\n",
    "    \n",
    "    if hands:\n",
    "        hand = hands[0]\n",
    "        x, y, w, h = hand['bbox']  # Get bounding box coordinates\n",
    "        \n",
    "        # Check if the hand is within the image boundaries\n",
    "        if x - offset >= 0 and y - offset >= 0 and x + w + offset < img.shape[1] and y + h + offset < img.shape[0]:\n",
    "            canvas = np.ones((imgSize, imgSize, 3), np.uint8) * 255  # Create a white canvas\n",
    "            imgCrop = img[y - offset:y + h + offset, x - offset:x + w + offset]  # Crop hand region\n",
    "            \n",
    "            # Resize and center the hand image within the canvas\n",
    "            aspectRatio = h / w\n",
    "            if aspectRatio > 1:\n",
    "                k = imgSize / h\n",
    "                wCal = math.ceil(k * w)\n",
    "                imgResize = cv2.resize(imgCrop, (wCal, imgSize))\n",
    "                wGap = math.ceil((imgSize - wCal) / 2)\n",
    "                canvas[:, wGap:wCal + wGap] = imgResize\n",
    "            else:\n",
    "                k = imgSize / w\n",
    "                hCal = math.ceil(k * h)\n",
    "                imgResize = cv2.resize(imgCrop, (imgSize, hCal))\n",
    "                hGap = math.ceil((imgSize - hCal) / 2)\n",
    "                canvas[hGap:hCal + hGap, :] = imgResize\n",
    "                \n",
    "            cv2.imshow(\"Gesture Monitor\", canvas)  # Display processed hand image\n",
    "            predicted_class_index = predict_gesture(canvas)  # Predict gesture class index\n",
    "            \n",
    "            # Retrieve the predicted gesture label from the list\n",
    "            class_label = labels_list[predicted_class_index]\n",
    "            cv2.putText(frame, class_label, (frame.shape[1] - 200, 50), cv2.FONT_HERSHEY_TRIPLEX, 1.5, (0, 255, 0), 2, cv2.FILLED)\n",
    "    \n",
    "    # Convert frame to RGB and display in tkinter window\n",
    "    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    img = Image.fromarray(frame)\n",
    "    img = ImageTk.PhotoImage(image=img)\n",
    "    panel.img = img\n",
    "    panel.config(image=img)\n",
    "    \n",
    "    # Schedule next frame update after specified time (milliseconds)\n",
    "    panel.after(1000 // FPS, show_frames)\n",
    "\n",
    "def quit():\n",
    "    # Release camera and close all windows\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    root.destroy()\n",
    "\n",
    "# Create main tkinter window\n",
    "root = tk.Tk()\n",
    "root.title(\"Sign Language Prediction\")\n",
    "\n",
    "# Create label to display video stream\n",
    "panel = tk.Label(root)\n",
    "panel.pack(padx=10, pady=10)\n",
    "\n",
    "# Configure window close event to call quit function\n",
    "root.protocol(\"WM_DELETE_WINDOW\", quit)\n",
    "\n",
    "# Start displaying video frames\n",
    "show_frames()\n",
    "\n",
    "# Start the tkinter main event loop\n",
    "root.mainloop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df3e849d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b328a83",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
